{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons and Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "Using the previously defined `Value` class, we can define a `Neuron` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"Class that represents a value that can be differentiated in respect to other values.\n",
    "\n",
    "    Attributes:\n",
    "        data: The value itself.\n",
    "        _prev: A set of values that this value is dependent on.\n",
    "        _op: The operation that produced this value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # derivative of the loss function in respect to this value\n",
    "        self._backward = lambda: None # defines how to propagate the output gradient to the input gradient\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value(data={self.data}, label={self.label})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        def _backward():\n",
    "            # For addition, the local derivative is always 1. So, given the expression a = b + c, it follows from the chain rule \n",
    "            # dz/dx = dz/dy*dy/dx that the gradients of b and c are (1.0 * the gradient of a)\n",
    "            if isinstance(other, Value):\n",
    "                self.grad += 1.0 * out.grad\n",
    "                other.grad += 1.0 * out.grad\n",
    "            else:\n",
    "                #print(f\"__add__ backward for {self} and {other}\")\n",
    "                #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "                self.grad += 1.0 * out.grad\n",
    "\n",
    "        if isinstance(other, Value):\n",
    "            out = Value(self.data + other.data, (self, other), '+')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            out = Value(self.data + other, (self,), '+')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "        \n",
    "    def __neg__(self): #-self\n",
    "        return self * -1\n",
    "        \n",
    "    def __sub__(self, other): #self - other, uses __neg__ and __add__\n",
    "        return self + (-other)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        def _backward():\n",
    "            # For multiplication, the local derivative is always the other value (because for z=x*y, dz/dx = y). \n",
    "            # So, given the expression a = b * c, it follows from the chain rule that the gradient of b\n",
    "            # is (c * the gradient of a), and the gradient of c is (b * the gradient of a).\n",
    "            if isinstance(other, Value):\n",
    "               self.grad += other.data * out.grad\n",
    "               other.grad += self.data * out.grad\n",
    "            else:\n",
    "                #print(f\"__mul__ backward for {self} and {other}\")\n",
    "                #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "                self.grad += other * out.grad\n",
    "\n",
    "        if isinstance(other, Value):\n",
    "            out = Value(self.data * other.data, (self, other), '*')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        else:\n",
    "            out = Value(self.data * other, (self,), '*')\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__pow__ backward for {self} and {other}\")\n",
    "            #print(f\"uid(self)={id(self)}, uid(other)={id(other)}\")\n",
    "            self.grad += (other * self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "        \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__tanh__ backward for {self}\")\n",
    "            #print(f\"uid(self)={id(self)}\")\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            #print(f\"__exp__ backward for {self}\")\n",
    "            #print(f\"uid(self)={id(self)}\")\n",
    "            self.grad += out.data * out.grad # because d/dx e^x = e^x\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            #print(f\"In Value.backward(): processing node {node} of type {type(node)}\")\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Value` class, we can now define the class `Neuron` to represent a single artificial neoron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \"\"\"Implementation of a neuron in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1), label='w') for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1), label='b')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # compute raw activation w * x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the `Neuron` class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.48798312459108867, label=)"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 6.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the `Neuron` class in place, we can define the class `Layer` to represent a layer of neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Implementation of a layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        \"\"\"Initialize a layer of nout neurons, each taking nin inputs.\"\"\"\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            params.extend(n.parameters())\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define an MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Implementation of a multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"Initialize a MLP with nouts layers, each taking nin inputs.\n",
    "\n",
    "        Args:\n",
    "            nin: Number of inputs to the MLP.\n",
    "            nouts: List of number of neurons in each layer (including the output layer).\n",
    "        \"\"\"\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we can create an MLP that has 3 inputs, to hidden layers with 4 neurons each, and 1 output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.0909697326091925, label=w),\n",
       " Value(data=0.6112772310149679, label=w),\n",
       " Value(data=-0.4577950619379323, label=w),\n",
       " Value(data=-0.5776340692060642, label=b),\n",
       " Value(data=-0.815249058822104, label=w),\n",
       " Value(data=0.2582770154305767, label=w),\n",
       " Value(data=-0.6390090586595913, label=w),\n",
       " Value(data=-0.8439652214931448, label=b),\n",
       " Value(data=0.810589835498601, label=w),\n",
       " Value(data=-0.5494748746516618, label=w),\n",
       " Value(data=0.3263442923256723, label=w),\n",
       " Value(data=-0.29442523991974445, label=b),\n",
       " Value(data=0.6143315480067737, label=w),\n",
       " Value(data=0.09891823289707613, label=w),\n",
       " Value(data=0.41786176911898165, label=w),\n",
       " Value(data=-0.9471203651062756, label=b),\n",
       " Value(data=-0.7248146719163313, label=w),\n",
       " Value(data=-0.6440168005187248, label=w),\n",
       " Value(data=0.006300007781024908, label=w),\n",
       " Value(data=-0.7186289410038025, label=w),\n",
       " Value(data=0.8821853381567681, label=b),\n",
       " Value(data=-0.3949357655976595, label=w),\n",
       " Value(data=-0.8590094734603584, label=w),\n",
       " Value(data=-0.987903179581803, label=w),\n",
       " Value(data=-0.8793238730705235, label=w),\n",
       " Value(data=-0.23481492639409574, label=b),\n",
       " Value(data=-0.8277752620387822, label=w),\n",
       " Value(data=-0.4339709552130364, label=w),\n",
       " Value(data=0.9803592285879963, label=w),\n",
       " Value(data=-0.41662788181861354, label=w),\n",
       " Value(data=0.13319077245418076, label=b),\n",
       " Value(data=0.16027716988336116, label=w),\n",
       " Value(data=0.05174782375986475, label=w),\n",
       " Value(data=0.8563145769840261, label=w),\n",
       " Value(data=-0.10093975737631644, label=w),\n",
       " Value(data=0.6397776433875715, label=b),\n",
       " Value(data=0.6054770458835417, label=w),\n",
       " Value(data=-0.6847007579208237, label=w),\n",
       " Value(data=0.494707751779631, label=w),\n",
       " Value(data=0.1673586950711916, label=w),\n",
       " Value(data=0.2579741908369322, label=b)]"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [4, 4, 1])\n",
    "m(x)\n",
    "m.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And draw the graph of the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualizing the graph\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # build a set of all nodes and edges in the graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # left to right\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangle node for it ('record')\n",
    "        dot.node(name = uid, label=f\"{n.label} | uid {uid} | data {n.data:.4f} | grad {n.grad:.4f}\", shape='record')\n",
    "        if n._op:\n",
    "            # if the value was produced by an operation, create an op node for it\n",
    "            dot.node(name = uid + n._op, label=n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2))+ n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dot(m(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform backpropagation using micrograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.05917979865995601, label=),\n",
       " Value(data=0.9512411366012725, label=),\n",
       " Value(data=0.6216821055349466, label=),\n",
       " Value(data=0.279251965353167, label=)]"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple dataset\n",
    "# Inputs\n",
    "xs = [\n",
    "    [2.0, 3.0,  -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "# Target outputs\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "# What predictions does the model currently make?\n",
    "ypred = [m(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the MLP, we need a single number to measure its performance and this number is called a _loss_. We will define loss in such a way so we can minimize it. Specifically, we will define it as the mean square error loss:\n",
    "\n",
    "$$\\sum_i (y_i - gt_i)^2$$\n",
    "\n",
    "where $y_i$ is the MLP's actual output for input vector $i$ and $gt_i$ is the ground truth for input vector $i$. For instance, in our case, $gt_0 = 1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=8.0785343999139, label=)"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3668324246648831"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run backpropagation ...\n",
    "loss.backward()\n",
    "\n",
    "# ... and see how the gradients look like\n",
    "m.layers[0].neurons[0].w[0].grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0909697326091925"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we would want to, we can plot the propagation graph for the loss function\n",
    "#draw_dot(loss)\n",
    "m.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent: minimize the loss by changing the parameters of the model. Since we want to minimize the loss,\n",
    "# we move in the opposite direction of the gradient (negative step size).\n",
    "for p in m.parameters():\n",
    "    p.data -= 0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08730140836254367"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=7.51487317615622, label=)"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now recalculate the loss\n",
    "ypred = [m(x) for x in xs]\n",
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.004208074840806357, label=)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do another step of gradient descent\n",
    "loss.backward()\n",
    "for p in m.parameters():\n",
    "    p.data -= 0.01 * p.grad # update the parameters (gradients)\n",
    "\n",
    "# and recalculate the loss\n",
    "ypred = [m(x) for x in xs]\n",
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9792078666872167, label=),\n",
       " Value(data=-0.9953641050390847, label=),\n",
       " Value(data=-0.9801373892305607, label=),\n",
       " Value(data=0.9420366736251022, label=)]"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What predictions does the model make now, after several rounds of gradient descent?\n",
    "# Remember that the target outputs are 1.0, -1.0, -1.0, 1.0\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we are doing in the _gradient descent_ is:\n",
    "1. Forward pass\n",
    "2. Backward pass\n",
    "3. Update the weights\n",
    "\n",
    "And we repeat this loop several times until we are happy with the value of the loss function. To demonstrate this, we'll have to reset the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Initial predictions: [Value(data=-0.19187537774717942, label=), Value(data=-0.08719664114242351, label=), Value(data=0.5661226649603694, label=), Value(data=-0.17506707327258142, label=)]\n",
      "0 6.0872997164136144\n",
      "1 2.8896979243934635\n",
      "2 1.2337072293917832\n",
      "3 0.45635016505112147\n",
      "4 0.25795472336832787\n",
      "5 0.1781367385667548\n",
      "6 0.1354958069762084\n",
      "7 0.10905133426223233\n",
      "8 0.09107949394578493\n",
      "9 0.07808638926105817\n",
      "10 0.06826411786855195\n",
      "11 0.060584153022008355\n",
      "12 0.05441857150716246\n",
      "13 0.04936248957433281\n",
      "14 0.04514320396201794\n",
      "15 0.04157037899468581\n",
      "16 0.03850716883539993\n",
      "17 0.03585268548460085\n",
      "18 0.03353093348953619\n",
      "19 0.031483590824624944\n",
      "20 0.02966516231873553\n",
      "21 0.028039643608578295\n",
      "22 0.026578173773429702\n",
      "23 0.025257351088858324\n",
      "24 0.02405800330991972\n",
      "25 0.022964275628664887\n",
      "26 0.021962944582417574\n",
      "27 0.021042895244345074\n",
      "28 0.020194718125998955\n",
      "29 0.01941039501423194\n",
      "30 0.018683051682985972\n",
      "31 0.018006761456314583\n",
      "32 0.017376387839038054\n",
      "33 0.01678745745016259\n",
      "34 0.016236056670271157\n",
      "35 0.01571874700093345\n",
      "36 0.015232495303841569\n",
      "37 0.014774615958213923\n",
      "38 0.014342722629490004\n",
      "39 0.013934687838551483\n",
      "40 0.013548608900057529\n",
      "41 0.013182779090755485\n",
      "42 0.012835663135473716\n",
      "43 0.012505876275793684\n",
      "44 0.012192166325869953\n",
      "45 0.011893398230273893\n",
      "46 0.01160854072665269\n",
      "47 0.011336654786396238\n",
      "48 0.011076883563180893\n",
      "49 0.010828443625120462\n",
      "50 0.010590617283545755\n",
      "51 0.010362745861894129\n",
      "52 0.010144223773186162\n",
      "53 0.009934493295160668\n",
      "54 0.009733039949179574\n",
      "55 0.009539388403165806\n",
      "56 0.00935309883063707\n",
      "57 0.009173763667769876\n",
      "58 0.009001004718716217\n",
      "59 0.008834470566377301\n",
      "60 0.008673834251738692\n",
      "61 0.008518791189871661\n",
      "62 0.00836905729495828\n",
      "63 0.008224367290322455\n",
      "64 0.008084473182548598\n",
      "65 0.007949142881426433\n",
      "66 0.00781815894974435\n",
      "67 0.007691317468921045\n",
      "68 0.007568427008164528\n",
      "69 0.007449307686318716\n",
      "70 0.007333790316834191\n",
      "71 0.007221715627409987\n",
      "72 0.0071129335468200355\n",
      "73 0.007007302552282654\n",
      "74 0.0069046890714708434\n",
      "75 0.006804966933907939\n",
      "76 0.006708016867064023\n",
      "77 0.006613726032966614\n",
      "78 0.006521987601583004\n",
      "79 0.006432700357619665\n",
      "80 0.006345768337729836\n",
      "81 0.006261100495424502\n",
      "82 0.006178610391255128\n",
      "83 0.0060982159060747835\n",
      "84 0.006019838975400775\n",
      "85 0.005943405343091774\n",
      "86 0.00586884433272376\n",
      "87 0.005796088635200794\n",
      "88 0.0057250741112740595\n",
      "89 0.005655739607764434\n",
      "90 0.005588026786393474\n",
      "91 0.0055218799642273095\n",
      "92 0.005457245964825626\n",
      "93 0.005394073979268779\n",
      "94 0.005332315436307806\n",
      "95 0.005271923880947575\n",
      "96 0.005212854860831781\n",
      "97 0.005155065819852406\n",
      "98 0.0050985159984539615\n",
      "99 0.005043166340147284\n",
      "[*] Final predictions: [Value(data=0.9659513705417588, label=), Value(data=-0.9647147617822062, label=), Value(data=-0.9661184924806571, label=), Value(data=0.9620966274860348, label=)]\n"
     ]
    }
   ],
   "source": [
    "m = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Inputs to the MLP\n",
    "xs = [\n",
    "    [2.0, 3.0,  -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "# Desired outputs (target outputs)\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "# Initial predictions of the MLP\n",
    "ypred = [m(x) for x in xs]\n",
    "print(f\"[*] Initial predictions: {ypred}\")\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "for k in range(100):\n",
    "    # forward pass\n",
    "    ypred = [m(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "\n",
    "    # backward pass (and don't forget to reset the gradients!)\n",
    "    # note to myself: if the problem is very simple, without resetting the gradients, the gradients will accumulate,\n",
    "    # but the descent will be faster...\n",
    "    for p in m.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in m.parameters():\n",
    "        p.data -= learning_rate * p.grad # update the parameters (gradients)\n",
    "\n",
    "    print(k, loss.data)\n",
    "\n",
    "# What predictions does the model make now, after k rounds of gradient descent?\n",
    "# Remember that the target outputs are 1.0, -1.0, -1.0, 1.0\n",
    "ypred = [m(x) for x in xs]\n",
    "print(f\"[*] Final predictions: {ypred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
